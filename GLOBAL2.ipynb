{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac76394e-3721-4a93-9e59-919ba43aea24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Preparing market data and impact matrices (pickle cache) ...\n",
      "\n",
      "[info] Starting scalability sweep (N = 2..50) ...\n",
      "[run ] N = 2 accounts ...\n",
      "[done] N=2: build=0.0503s, solve=0.0074s, total=0.0577s, status=ok\n",
      "[run ] N = 3 accounts ...\n",
      "[done] N=3: build=0.0651s, solve=0.0102s, total=0.0754s, status=ok\n",
      "[run ] N = 4 accounts ...\n",
      "[done] N=4: build=0.0648s, solve=0.0124s, total=0.0772s, status=ok\n",
      "[run ] N = 5 accounts ...\n",
      "[done] N=5: build=0.1191s, solve=0.0161s, total=0.1353s, status=ok\n",
      "[run ] N = 6 accounts ...\n",
      "[done] N=6: build=0.1630s, solve=0.0230s, total=0.1860s, status=ok\n",
      "[run ] N = 7 accounts ...\n",
      "[done] N=7: build=0.1607s, solve=0.0264s, total=0.1871s, status=ok\n",
      "[run ] N = 8 accounts ...\n",
      "[done] N=8: build=0.1809s, solve=0.0276s, total=0.2085s, status=ok\n",
      "[run ] N = 9 accounts ...\n",
      "[done] N=9: build=0.1937s, solve=0.0391s, total=0.2328s, status=ok\n",
      "[run ] N = 10 accounts ...\n",
      "[done] N=10: build=0.2517s, solve=0.0404s, total=0.2921s, status=ok\n",
      "[run ] N = 11 accounts ...\n",
      "[done] N=11: build=0.2672s, solve=0.0460s, total=0.3132s, status=ok\n",
      "[run ] N = 12 accounts ...\n",
      "[done] N=12: build=0.2884s, solve=0.0483s, total=0.3367s, status=ok\n",
      "[run ] N = 13 accounts ...\n",
      "[done] N=13: build=0.3149s, solve=0.0623s, total=0.3771s, status=ok\n",
      "[run ] N = 14 accounts ...\n",
      "[done] N=14: build=0.3074s, solve=0.0632s, total=0.3706s, status=ok\n",
      "[run ] N = 15 accounts ...\n",
      "[done] N=15: build=0.3309s, solve=0.0739s, total=0.4048s, status=ok\n",
      "[run ] N = 16 accounts ...\n",
      "[done] N=16: build=0.4058s, solve=0.0944s, total=0.5002s, status=ok\n",
      "[run ] N = 17 accounts ...\n",
      "[done] N=17: build=0.4206s, solve=0.1000s, total=0.5206s, status=ok\n",
      "[run ] N = 18 accounts ...\n",
      "[done] N=18: build=0.4480s, solve=0.1035s, total=0.5515s, status=ok\n",
      "[run ] N = 19 accounts ...\n",
      "[done] N=19: build=0.4678s, solve=0.1212s, total=0.5890s, status=ok\n",
      "[run ] N = 20 accounts ...\n",
      "[done] N=20: build=0.5092s, solve=0.1192s, total=0.6284s, status=ok\n",
      "[run ] N = 21 accounts ...\n",
      "[done] N=21: build=0.5599s, solve=0.1397s, total=0.6996s, status=ok\n",
      "[run ] N = 22 accounts ...\n",
      "[done] N=22: build=0.6176s, solve=0.2203s, total=0.8379s, status=ok\n",
      "[run ] N = 23 accounts ...\n",
      "[done] N=23: build=0.6610s, solve=0.2546s, total=0.9156s, status=ok\n",
      "[run ] N = 24 accounts ...\n",
      "[done] N=24: build=0.7038s, solve=0.2660s, total=0.9698s, status=ok\n",
      "[run ] N = 25 accounts ...\n",
      "[done] N=25: build=0.7642s, solve=0.2856s, total=1.0498s, status=ok\n",
      "[run ] N = 26 accounts ...\n",
      "[done] N=26: build=0.8218s, solve=0.3130s, total=1.1347s, status=ok\n",
      "[run ] N = 27 accounts ...\n",
      "[done] N=27: build=0.8881s, solve=0.3382s, total=1.2264s, status=ok\n",
      "[run ] N = 28 accounts ...\n",
      "[done] N=28: build=1.0063s, solve=0.3596s, total=1.3659s, status=ok\n",
      "[run ] N = 29 accounts ...\n",
      "[done] N=29: build=1.0320s, solve=0.3885s, total=1.4206s, status=ok\n",
      "[run ] N = 30 accounts ...\n",
      "[done] N=30: build=1.1005s, solve=0.3933s, total=1.4938s, status=ok\n",
      "[run ] N = 31 accounts ...\n",
      "[done] N=31: build=1.1757s, solve=0.4405s, total=1.6162s, status=ok\n",
      "[run ] N = 32 accounts ...\n",
      "[done] N=32: build=1.2298s, solve=0.4850s, total=1.7148s, status=ok\n",
      "[run ] N = 33 accounts ...\n",
      "[done] N=33: build=1.2726s, solve=0.5149s, total=1.7875s, status=ok\n",
      "[run ] N = 34 accounts ...\n",
      "[done] N=34: build=1.4062s, solve=0.5650s, total=1.9712s, status=ok\n",
      "[run ] N = 35 accounts ...\n",
      "[done] N=35: build=1.4341s, solve=0.5880s, total=2.0221s, status=ok\n",
      "[run ] N = 36 accounts ...\n",
      "[done] N=36: build=1.4792s, solve=0.6335s, total=2.1127s, status=ok\n",
      "[run ] N = 37 accounts ...\n",
      "[done] N=37: build=1.5446s, solve=0.6765s, total=2.2211s, status=ok\n",
      "[run ] N = 38 accounts ...\n",
      "[done] N=38: build=1.6623s, solve=0.7638s, total=2.4262s, status=ok\n",
      "[run ] N = 39 accounts ...\n",
      "[done] N=39: build=1.7979s, solve=0.7807s, total=2.5786s, status=ok\n",
      "[run ] N = 40 accounts ...\n",
      "[done] N=40: build=1.8008s, solve=0.8048s, total=2.6057s, status=ok\n",
      "[run ] N = 41 accounts ...\n",
      "[done] N=41: build=1.8529s, solve=0.8432s, total=2.6961s, status=ok\n",
      "[run ] N = 42 accounts ...\n",
      "[done] N=42: build=2.0394s, solve=1.0816s, total=3.1210s, status=ok\n",
      "[run ] N = 43 accounts ...\n",
      "[done] N=43: build=2.1508s, solve=1.0622s, total=3.2130s, status=ok\n",
      "[run ] N = 44 accounts ...\n",
      "[done] N=44: build=2.2582s, solve=1.2107s, total=3.4689s, status=ok\n",
      "[run ] N = 45 accounts ...\n",
      "[done] N=45: build=2.5092s, solve=1.1946s, total=3.7038s, status=ok\n",
      "[run ] N = 46 accounts ...\n",
      "[done] N=46: build=2.6429s, solve=1.1696s, total=3.8125s, status=ok\n",
      "[run ] N = 47 accounts ...\n",
      "[done] N=47: build=2.7634s, solve=1.2898s, total=4.0532s, status=ok\n",
      "[run ] N = 48 accounts ...\n",
      "[done] N=48: build=2.7157s, solve=1.3427s, total=4.0584s, status=ok\n",
      "[run ] N = 49 accounts ...\n",
      "[done] N=49: build=2.7128s, solve=1.4165s, total=4.1293s, status=ok\n",
      "[run ] N = 50 accounts ...\n",
      "[done] N=50: build=2.9928s, solve=1.4590s, total=4.4517s, status=ok\n",
      "\n",
      "[info] Runtime summary saved to: scalability_test_2_to_50\\runtime_summary.csv\n",
      "[info] Plot saved to: scalability_test_2_to_50\\runtime_vs_accounts.png\n",
      "\n",
      "All scalability tests finished.\n",
      "Outputs are in: C:\\Users\\s2667242\\Desktop\\MyNotebooks\\anaconda_projects\\db\\scalability_test_2_to_50\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Scalability test — single-period InfoPool (fixed τ, 20 assets)\n",
    "Scale N from 2 to 50 accounts.\n",
    "Accounts 11..50 replicate info/constraints of 1..10 (pattern by n % 10).\n",
    "No FairSplit / rebates here.\n",
    "\n",
    "Outputs (in scalability_test_2_to_50/):\n",
    "  - runtime_summary.csv   (N, build_time, solve_time, total_time, status)\n",
    "  - runtime_vs_accounts.png\n",
    "  - cached market data / matrices under _cache/ (pickle/npz, no pyarrow needed)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "import xpress as xp\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Paths & switches\n",
    "# -----------------------------\n",
    "OUTDIR = \"scalability_test_2_to_50\"\n",
    "CACHE_DIR = os.path.join(OUTDIR, \"_cache\")\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "ANNUAL_TRADING_DAYS = 252\n",
    "\n",
    "# -----------------------------\n",
    "# Xpress init (guarded)\n",
    "# -----------------------------\n",
    "XPRESS_LICENSE_PATH = r\"C:/Users/s2667242/AppData/Local/anaconda3/Lib/site-packages/xpress/license/xpauth.xpr\"\n",
    "\n",
    "def ensure_xpress_inited():\n",
    "    if getattr(xp, \"_JYH_ALREADY_INITED\", False):\n",
    "        return\n",
    "    try:\n",
    "        xp.init(XPRESS_LICENSE_PATH)\n",
    "    except Exception as e:\n",
    "        print(\"[warn] xp.init() failed or already initialized, proceeding if environment is live:\", e)\n",
    "    try:\n",
    "        with xp.problem() as _p:\n",
    "            _p.setObjective(1.0, sense=xp.maximize)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\"Xpress global environment not available; restart kernel or fix license path.\") from e\n",
    "    xp._JYH_ALREADY_INITED = True\n",
    "\n",
    "ensure_xpress_inited()\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Market data & Ω-construction  (with pickle cache)\n",
    "# -----------------------------\n",
    "tickers = [\n",
    "    \"AAPL\",\"MSFT\",\"GOOGL\",\"AMZN\",\"META\",\"TSLA\",\"JNJ\",\"JPM\",\"V\",\"PG\",\n",
    "    \"NVDA\",\"DIS\",\"UNH\",\"HD\",\"MA\",\"PFE\",\"BAC\",\"KO\",\"XOM\",\"CVX\"\n",
    "]\n",
    "d = len(tickers)\n",
    "\n",
    "start_date = \"2020-01-01\"\n",
    "end_date   = \"2025-07-24\"\n",
    "\n",
    "prices_adj_pkl = os.path.join(CACHE_DIR, \"prices_adj.pkl\")\n",
    "prices_raw_pkl = os.path.join(CACHE_DIR, \"prices_raw.pkl\")\n",
    "vols_pkl       = os.path.join(CACHE_DIR, \"volumes.pkl\")\n",
    "omega_npz      = os.path.join(CACHE_DIR, \"omegas_corr.npz\")\n",
    "\n",
    "def winsorize_ser(s: pd.Series, q=0.995):\n",
    "    lo, hi = s.quantile(1-q), s.quantile(q)\n",
    "    return s.clip(lower=lo, upper=hi)\n",
    "\n",
    "def estimate_kappa_from_illiq(prices_raw, volumes, returns, sigma_vec, side=\"both\"):\n",
    "    px_raw = prices_raw.loc[returns.index]\n",
    "    vol    = volumes.loc[returns.index]\n",
    "    dollar_vol = (px_raw * vol).replace(0, np.nan)\n",
    "    illiq = (returns.abs() / dollar_vol).replace([np.inf, -np.inf], np.nan)\n",
    "    if side == \"plus\":\n",
    "        mask = (returns > 0)\n",
    "    elif side == \"minus\":\n",
    "        mask = (returns < 0)\n",
    "    else:\n",
    "        mask = pd.DataFrame(True, index=returns.index, columns=returns.columns)\n",
    "    illiq_med = winsorize_ser(illiq.where(mask).median(axis=0, skipna=True), q=0.995)\n",
    "    Vbar_local = (px_raw * vol).mean(axis=0)\n",
    "    sigma = pd.Series(sigma_vec, index=returns.columns)\n",
    "    kappa = (illiq_med * Vbar_local / sigma).replace([np.inf, -np.inf], np.nan)\n",
    "    return kappa.clip(lower=0.01, upper=100.0).values\n",
    "\n",
    "def gershgorin_delta_max(gamma: np.ndarray, corr: np.ndarray):\n",
    "    G = np.sqrt(np.outer(gamma, gamma))\n",
    "    A = np.abs(corr) * G\n",
    "    row_sum_off = A.sum(axis=1) - np.diag(A)\n",
    "    row_sum_off = np.where(row_sum_off <= 1e-12, np.inf, row_sum_off)\n",
    "    return float(np.min(gamma / row_sum_off))\n",
    "\n",
    "def delta_from_ratio(gamma: np.ndarray, corr: np.ndarray, rho=0.3, cap_ratio=0.9):\n",
    "    G = np.sqrt(np.outer(gamma, gamma))\n",
    "    A = np.abs(corr) * G\n",
    "    row_sum_off = A.sum(axis=1) - np.diag(A)\n",
    "    factor = row_sum_off / gamma\n",
    "    mf = np.mean(factor[np.isfinite(factor) & (factor > 0)])\n",
    "    delta_star = rho / mf if (mf is not None and mf > 0) else 0.0\n",
    "    delta_max  = gershgorin_delta_max(gamma, corr)\n",
    "    return float(min(delta_star, cap_ratio * delta_max)), float(delta_max)\n",
    "\n",
    "print(\"[info] Preparing market data and impact matrices (pickle cache) ...\")\n",
    "need_download = not (os.path.exists(prices_adj_pkl) and os.path.exists(prices_raw_pkl) and os.path.exists(vols_pkl))\n",
    "if need_download:\n",
    "    data = yf.download(tickers, start=start_date, end=end_date, auto_adjust=False, progress=False)\n",
    "    prices_adj = data[\"Adj Close\"]\n",
    "    prices_raw = data[\"Close\"]\n",
    "    volumes    = data[\"Volume\"]\n",
    "    prices_adj.to_pickle(prices_adj_pkl)\n",
    "    prices_raw.to_pickle(prices_raw_pkl)\n",
    "    volumes.to_pickle(vols_pkl)\n",
    "else:\n",
    "    prices_adj = pd.read_pickle(prices_adj_pkl)\n",
    "    prices_raw = pd.read_pickle(prices_raw_pkl)\n",
    "    volumes    = pd.read_pickle(vols_pkl)\n",
    "\n",
    "returns = prices_adj.pct_change().dropna()\n",
    "mean_daily_returns = returns.mean()\n",
    "cov_matrix         = returns.cov()\n",
    "\n",
    "sigma_all   = returns.std(axis=0).values\n",
    "up_returns  = returns.clip(lower=0)\n",
    "down_returns= (-returns).clip(lower=0)\n",
    "sigma_plus  = up_returns.std(axis=0).values\n",
    "sigma_minus = down_returns.std(axis=0).values\n",
    "\n",
    "dollar_vol = (prices_raw.reindex_like(volumes) * volumes).loc[returns.index]\n",
    "Vbar = dollar_vol.mean(axis=0).values\n",
    "\n",
    "if os.path.exists(omega_npz):\n",
    "    npz = np.load(omega_npz)\n",
    "    Omega_plus  = npz[\"Op\"]\n",
    "    Omega_minus = npz[\"Om\"]\n",
    "    corr        = npz[\"corr\"]\n",
    "else:\n",
    "    corr = cov_matrix.values / np.outer(sigma_all, sigma_all)\n",
    "    kappa_plus_hat  = estimate_kappa_from_illiq(prices_raw, volumes, returns, sigma_plus,  side=\"plus\")\n",
    "    kappa_minus_hat = estimate_kappa_from_illiq(prices_raw, volumes, returns, sigma_minus, side=\"minus\")\n",
    "    gamma_plus  = kappa_plus_hat  * sigma_plus  / Vbar\n",
    "    gamma_minus = kappa_minus_hat * sigma_minus / Vbar\n",
    "    delta_plus,  _ = delta_from_ratio(gamma_plus,  corr, rho=0.3, cap_ratio=0.9)\n",
    "    delta_minus, _ = delta_from_ratio(gamma_minus, corr, rho=0.3, cap_ratio=0.9)\n",
    "    Gp = np.sqrt(np.outer(gamma_plus,  gamma_plus))\n",
    "    Gm = np.sqrt(np.outer(gamma_minus, gamma_minus))\n",
    "    Omega_plus  = np.diag(gamma_plus)  + delta_plus  * (Gp * corr)\n",
    "    Omega_minus = np.diag(gamma_minus) + delta_minus * (Gm * corr)\n",
    "    # scale to target bps\n",
    "    xi = 0.10\n",
    "    target_bps = 100\n",
    "    c = target_bps / 1e4  # 100 bps = 0.01\n",
    "    s_cand_p = 2*c / np.maximum(gamma_plus  * xi * Vbar, 1e-30)\n",
    "    s_cand_m = 2*c / np.maximum(gamma_minus * xi * Vbar, 1e-30)\n",
    "    s_omega  = float(np.median(np.concatenate([s_cand_p, s_cand_m])))\n",
    "    Omega_plus  = s_omega * Omega_plus\n",
    "    Omega_minus = s_omega * Omega_minus\n",
    "    np.savez_compressed(omega_npz, Op=Omega_plus, Om=Omega_minus, corr=corr)\n",
    "\n",
    "for name, M in [(\"Omega_plus\", Omega_plus), (\"Omega_minus\", Omega_minus)]:\n",
    "    if not np.isfinite(M).all():\n",
    "        raise ValueError(f\"{name} contains non-finite values.\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Build up to 50 accounts by replicating the first 10\n",
    "# -----------------------------\n",
    "N_MAX = 50     # scale to 50\n",
    "BASE = 10      # replicate every 10 accounts (pattern by n % 10)\n",
    "\n",
    "gamma_core = [\n",
    "    0.49671415, 0.64768854, 0.76743473, 0.54256004, 0.24196227,\n",
    "    0.31424733, 0.06752820, 0.11092259, 0.37569802, 0.82254491\n",
    "]\n",
    "budgets_core = [1e6, 5e5, 8e5, 1.2e6, 6e5, 9e5, 7e5, 4e5, 1.5e6, 1e6]\n",
    "\n",
    "# repeat core pattern to length 50\n",
    "repeat_times = (N_MAX + BASE - 1) // BASE  # ceil(50/10)=5\n",
    "gamma_all   = (gamma_core   * repeat_times)[:N_MAX]\n",
    "budgets_all = (budgets_core * repeat_times)[:N_MAX]\n",
    "\n",
    "trading_days = ANNUAL_TRADING_DAYS\n",
    "mu_vec      = mean_daily_returns.values\n",
    "Sigma_mat   = cov_matrix.values\n",
    "mu_annual   = mu_vec * trading_days\n",
    "Sigma_annual= Sigma_mat * trading_days\n",
    "\n",
    "mu_list_ALL    = [mu_annual.copy()    for _ in range(N_MAX)]\n",
    "Sigma_list_ALL = [Sigma_annual.copy() for _ in range(N_MAX)]\n",
    "\n",
    "# risk scaling θ (pattern replicated; common scale keeps pattern invariant)\n",
    "w_pct0 = np.full((N_MAX, d), 1.0/d)\n",
    "risk_proto = np.array([\n",
    "    0.5 * gamma_all[n] * (budgets_all[n]**2) * (w_pct0[n] @ Sigma_list_ALL[n] @ w_pct0[n])\n",
    "    for n in range(N_MAX)\n",
    "])\n",
    "rev_est = np.array([budgets_all[n] * (mu_list_ALL[n] @ w_pct0[n]) for n in range(N_MAX)])\n",
    "alpha_risk = 0.5\n",
    "s_theta = (alpha_risk * max(rev_est.mean(), 1e-12)) / max(risk_proto.mean(), 1e-12)\n",
    "theta_list_ALL = (s_theta * np.array(gamma_all)).tolist()\n",
    "\n",
    "# initial holdings: build deterministic base-10 and replicate to 50\n",
    "INIT_HOLDINGS_PATH = os.path.join(CACHE_DIR, \"initial_holdings_50rep.csv\")\n",
    "if os.path.exists(INIT_HOLDINGS_PATH):\n",
    "    old_holdings_ALL = pd.read_csv(INIT_HOLDINGS_PATH, index_col=0).values\n",
    "    if old_holdings_ALL.shape != (N_MAX, d):\n",
    "        os.remove(INIT_HOLDINGS_PATH)\n",
    "        old_holdings_ALL = None\n",
    "else:\n",
    "    old_holdings_ALL = None\n",
    "\n",
    "if old_holdings_ALL is None:\n",
    "    def _dirichlet_like_from_hash(tickers, budgets, seed=12345):\n",
    "        Nn, dd = len(budgets), len(tickers)\n",
    "        W = np.zeros((Nn, dd), dtype=float)\n",
    "        for n in range(Nn):\n",
    "            e = np.empty(dd, dtype=float)\n",
    "            for j, t in enumerate(tickers):\n",
    "                key = f\"{seed}|acct={n}|ticker={t}\".encode()\n",
    "                h = hashlib.sha256(key).digest()\n",
    "                u64 = int.from_bytes(h[:8], \"big\")\n",
    "                u = (u64 + 0.5) / 2**64\n",
    "                u = min(max(u, 1e-12), 1-1e-12)\n",
    "                e[j] = -np.log(u)\n",
    "            w = e / e.sum()\n",
    "            W[n, :] = w\n",
    "        B = np.asarray(budgets, dtype=float)[:, None]\n",
    "        return W * B\n",
    "\n",
    "    base10 = _dirichlet_like_from_hash(tickers, budgets_core, seed=12345)  # (10×20)\n",
    "    old_holdings_ALL = np.vstack([base10 for _ in range(repeat_times)])[:N_MAX, :]  # (50×20)\n",
    "    pd.DataFrame(old_holdings_ALL,\n",
    "                 index=[f\"Acct {i}\" for i in range(N_MAX)],\n",
    "                 columns=tickers).to_csv(INIT_HOLDINGS_PATH, float_format=\"%.6f\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Fixed τ (N×20): Acct 0 never shares; others share\n",
    "# -----------------------------\n",
    "TAU_BASE = np.ones((N_MAX, d), dtype=int)\n",
    "TAU_BASE[0, :] = 0\n",
    "pd.DataFrame(TAU_BASE, index=[f\"Acct {i}\" for i in range(N_MAX)], columns=tickers)\\\n",
    "  .to_csv(os.path.join(OUTDIR, \"tau_base_used.csv\"), float_format=\"%.0f\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Constraints replicator: account n uses pattern of (n % 10)\n",
    "# -----------------------------\n",
    "SECTOR_IDXS = {\n",
    "    \"T\":  [tickers.index(t) for t in [\"AAPL\",\"MSFT\",\"NVDA\"]],\n",
    "    \"C\":  [tickers.index(t) for t in [\"GOOGL\",\"META\",\"DIS\"]],\n",
    "    \"R\":  [tickers.index(t) for t in [\"AMZN\",\"HD\",\"TSLA\"]],\n",
    "    \"H\":  [tickers.index(t) for t in [\"JNJ\",\"UNH\",\"PFE\"]],\n",
    "    \"F\":  [tickers.index(t) for t in [\"JPM\",\"BAC\",\"V\",\"MA\"]],\n",
    "    \"S\":  [tickers.index(t) for t in [\"PG\",\"KO\"]],\n",
    "    \"E\":  [tickers.index(t) for t in [\"XOM\",\"CVX\"]],\n",
    "}\n",
    "GROWTH8 = [tickers.index(t) for t in [\"NVDA\",\"TSLA\",\"AAPL\",\"MSFT\"]]\n",
    "\n",
    "def add_constraints_for_account(model, n, w, budgets):\n",
    "    p = n % 10\n",
    "    if p == 0:\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in SECTOR_IDXS[\"S\"]+SECTOR_IDXS[\"H\"]) >=  budgets[n]*0.40)\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in SECTOR_IDXS[\"T\"]+SECTOR_IDXS[\"R\"]) <=  budgets[n]*0.25)\n",
    "    elif p == 1:\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in SECTOR_IDXS[\"T\"]) >=  budgets[n]*0.40)\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in SECTOR_IDXS[\"C\"]+[tickers.index(\"AMZN\")]) >= budgets[n]*0.30)\n",
    "    elif p == 2:\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in SECTOR_IDXS[\"T\"]) >=  budgets[n]*0.10)\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in SECTOR_IDXS[\"T\"]) <=  budgets[n]*0.25)\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in SECTOR_IDXS[\"C\"]) >=  budgets[n]*0.10)\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in SECTOR_IDXS[\"C\"]) <=  budgets[n]*0.25)\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in SECTOR_IDXS[\"H\"]) >=  budgets[n]*0.10)\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in SECTOR_IDXS[\"H\"]) <=  budgets[n]*0.25)\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in SECTOR_IDXS[\"F\"]) >=  budgets[n]*0.10)\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in SECTOR_IDXS[\"F\"]) <=  budgets[n]*0.25)\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in SECTOR_IDXS[\"S\"]+SECTOR_IDXS[\"E\"]) >= budgets[n]*0.05)\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in SECTOR_IDXS[\"S\"]+SECTOR_IDXS[\"E\"]) <= budgets[n]*0.15)\n",
    "    elif p == 3:\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in SECTOR_IDXS[\"S\"]+SECTOR_IDXS[\"F\"]+SECTOR_IDXS[\"E\"]) >= budgets[n]*0.50)\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in SECTOR_IDXS[\"F\"]) >=  budgets[n]*0.20)\n",
    "    elif p == 4:\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in SECTOR_IDXS[\"E\"]) >=  budgets[n]*0.15)\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in SECTOR_IDXS[\"F\"]) >=  budgets[n]*0.25)\n",
    "    elif p == 5:\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in SECTOR_IDXS[\"T\"]) >=  budgets[n]*0.30)\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in SECTOR_IDXS[\"H\"]) >=  budgets[n]*0.20)\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in SECTOR_IDXS[\"S\"]+SECTOR_IDXS[\"R\"]) <= budgets[n]*0.15)\n",
    "    elif p == 6:\n",
    "        growth6 = SECTOR_IDXS[\"T\"] + SECTOR_IDXS[\"C\"] + SECTOR_IDXS[\"R\"]\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in growth6) <= budgets[n]*0.35)\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in SECTOR_IDXS[\"E\"]+SECTOR_IDXS[\"F\"]+SECTOR_IDXS[\"S\"]) >= budgets[n]*0.40)\n",
    "    elif p == 7:\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in [tickers.index(\"AMZN\"), tickers.index(\"TSLA\")]) >= budgets[n]*0.30)\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in SECTOR_IDXS[\"T\"]+SECTOR_IDXS[\"F\"]+SECTOR_IDXS[\"H\"]) <= budgets[n]*0.50)\n",
    "    elif p == 8:\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in GROWTH8) >= budgets[n]*0.50)\n",
    "    elif p == 9:\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in SECTOR_IDXS[\"S\"]+SECTOR_IDXS[\"E\"]) >= budgets[n]*0.20)\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in SECTOR_IDXS[\"T\"]+SECTOR_IDXS[\"C\"]+SECTOR_IDXS[\"R\"]) >= budgets[n]*0.30)\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Core solver (build-time & solve-time separated)\n",
    "# -----------------------------\n",
    "def solve_infopool_once(N: int, tau_mat: np.ndarray,\n",
    "                        budgets, mu_list, Sigma_list,\n",
    "                        Omega_plus, Omega_minus,\n",
    "                        old_holdings, theta_list):\n",
    "    assert tau_mat.shape == (N, d)\n",
    "    assert old_holdings.shape == (N, d)\n",
    "\n",
    "    with xp.problem() as model:\n",
    "        model.controls.outputlog = 0  # faster; comment out to see logs\n",
    "        # model.controls.threads = -1  # optional: all threads\n",
    "\n",
    "        tb0 = time.perf_counter()\n",
    "\n",
    "        # variables\n",
    "        w   = [[model.addVariable(lb=0.0, ub=budgets[n]) for j in range(d)] for n in range(N)]\n",
    "        wp  = [[model.addVariable(lb=0.0, ub=budgets[n]) for j in range(d)] for n in range(N)]\n",
    "        wm  = [[model.addVariable(lb=0.0, ub=budgets[n]) for j in range(d)] for n in range(N)]\n",
    "        w_pct = [[model.addVariable(lb=0.0, ub=1.0)     for j in range(d)] for n in range(N)]\n",
    "\n",
    "        # buy/sell split\n",
    "        for n in range(N):\n",
    "            for j in range(d):\n",
    "                model.addConstraint(wp[n][j] - wm[n][j] == w[n][j] - old_holdings[n][j])\n",
    "\n",
    "        # budgets + simplex\n",
    "        for n in range(N):\n",
    "            model.addConstraint(xp.Sum(w[n][j] for j in range(d)) == budgets[n])\n",
    "            for j in range(d):\n",
    "                model.addConstraint(w[n][j] == budgets[n] * w_pct[n][j])\n",
    "            model.addConstraint(xp.Sum(w_pct[n][j] for j in range(d)) == 1)\n",
    "\n",
    "        # replicate constraints by pattern p = n % 10\n",
    "        for n in range(N):\n",
    "            add_constraints_for_account(model, n, w, budgets)\n",
    "\n",
    "        # objective\n",
    "        lin = xp.Sum(mu_list[n][j] * w[n][j] for n in range(N) for j in range(d))\n",
    "        risk = xp.Sum(\n",
    "            -0.5 * theta_list[n] * (budgets[n]**2) *\n",
    "            xp.Sum(Sigma_list[n][i][j] * w_pct[n][i] * w_pct[n][j] for i in range(d) for j in range(d))\n",
    "            for n in range(N)\n",
    "        )\n",
    "\n",
    "        tau_arr = np.asarray(tau_mat, dtype=int)\n",
    "        T_plus  = [xp.Sum(tau_arr[k, j] * wp[k][j]  for k in range(N)) for j in range(d)]\n",
    "        T_minus = [xp.Sum(tau_arr[k, j] * wm[k][j] for k in range(N)) for j in range(d)]\n",
    "\n",
    "        quad_pp = xp.Sum(T_plus[i]  * xp.Sum(Omega_plus[i,  j] * T_plus[j]  for j in range(d)) for i in range(d))\n",
    "        quad_mm = xp.Sum(T_minus[i] * xp.Sum(Omega_plus[i,  j] * T_minus[j] for j in range(d)) for i in range(d))\n",
    "        cross_pm= xp.Sum(T_plus[i]  * xp.Sum(Omega_minus[i, j] * T_minus[j] for j in range(d)) for i in range(d))\n",
    "        impact_pool = 0.5 * (quad_pp + quad_mm + 2 * cross_pm)\n",
    "\n",
    "        impact_ind = 0\n",
    "        for m in range(N):\n",
    "            r_p = [ (1 - tau_arr[m, j]) * wp[m][j]  for j in range(d) ]\n",
    "            r_m = [ (1 - tau_arr[m, j]) * wm[m][j] for j in range(d) ]\n",
    "            quad_pp_m = xp.Sum(r_p[i] * xp.Sum(Omega_plus[i,  j] * r_p[j] for j in range(d)) for i in range(d))\n",
    "            quad_mm_m = xp.Sum(r_m[i] * xp.Sum(Omega_plus[i,  j] * r_m[j] for j in range(d)) for i in range(d))\n",
    "            cross_pm_m= xp.Sum(r_p[i] * xp.Sum(Omega_minus[i, j] * r_m[j] for j in range(d)) for i in range(d))\n",
    "            impact_ind += 0.5 * (quad_pp_m + quad_mm_m + 2.0 * cross_pm_m)\n",
    "\n",
    "        impact = - ANNUAL_TRADING_DAYS * (impact_pool + impact_ind)\n",
    "        model.setObjective(lin + risk + impact, sense=xp.maximize)\n",
    "\n",
    "        tb1 = time.perf_counter()\n",
    "        model.solve()\n",
    "        ts1 = time.perf_counter()\n",
    "\n",
    "        build_time = tb1 - tb0\n",
    "        solve_time = ts1 - tb1\n",
    "\n",
    "    return build_time, solve_time\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Run scalability sweep: N = 2..50\n",
    "# -----------------------------\n",
    "runtime_records = []\n",
    "print(\"\\n[info] Starting scalability sweep (N = 2..50) ...\")\n",
    "\n",
    "for N in range(2, N_MAX+1):\n",
    "    print(f\"[run ] N = {N} accounts ...\")\n",
    "\n",
    "    budgets     = budgets_all[:N]\n",
    "    mu_list     = mu_list_ALL[:N]\n",
    "    Sigma_list  = Sigma_list_ALL[:N]\n",
    "    theta_list  = theta_list_ALL[:N]\n",
    "    old_holdings= old_holdings_ALL[:N, :]\n",
    "    tau_N       = TAU_BASE[:N, :].copy()\n",
    "\n",
    "    try:\n",
    "        build_t, solve_t = solve_infopool_once(\n",
    "            N, tau_N, budgets, mu_list, Sigma_list,\n",
    "            Omega_plus, Omega_minus, old_holdings, theta_list\n",
    "        )\n",
    "        total_t = build_t + solve_t\n",
    "        status = \"ok\"\n",
    "    except Exception as e:\n",
    "        build_t = np.nan; solve_t = np.nan; total_t = np.nan\n",
    "        status = f\"error: {e}\"\n",
    "        print(f\"[ERR ] N={N}: {e}\")\n",
    "\n",
    "    runtime_records.append({\n",
    "        \"N\": N,\n",
    "        \"build_time_sec\": build_t,\n",
    "        \"solve_time_sec\": solve_t,\n",
    "        \"total_time_sec\": total_t,\n",
    "        \"status\": status\n",
    "    })\n",
    "    print(f\"[done] N={N}: build={build_t:.4f}s, solve={solve_t:.4f}s, total={total_t:.4f}s, status={status}\")\n",
    "\n",
    "# Master runtime index\n",
    "rt_df = pd.DataFrame(runtime_records)\n",
    "rt_path = os.path.join(OUTDIR, \"runtime_summary.csv\")\n",
    "rt_df.to_csv(rt_path, index=False)\n",
    "print(\"\\n[info] Runtime summary saved to:\", rt_path)\n",
    "\n",
    "# -----------------------------\n",
    "# 7) Visualization\n",
    "# -----------------------------\n",
    "ok_df = rt_df[rt_df[\"status\"] == \"ok\"].copy()\n",
    "plt.figure(figsize=(7.2, 4.4))\n",
    "plt.plot(ok_df[\"N\"], ok_df[\"total_time_sec\"], marker=\"o\")\n",
    "plt.xlabel(\"Number of Accounts (N)\")\n",
    "plt.ylabel(\"Total Time (seconds)\")\n",
    "plt.title(\"InfoPool Single-Period Scalability (Assets = 20): N=2..50\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "png_path = os.path.join(OUTDIR, \"runtime_vs_accounts.png\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(png_path, dpi=180)\n",
    "plt.close()\n",
    "print(\"[info] Plot saved to:\", png_path)\n",
    "\n",
    "print(\"\\nAll scalability tests finished.\")\n",
    "print(f\"Outputs are in: {os.path.abspath(OUTDIR)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0ed9803-c094-4a55-b60d-7f86e221a46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Preparing market data and impact matrices (pickle cache) ...\n",
      "\n",
      "[info] Starting scalability sweep with repeats (N = 2..50, K = 10) ...\n",
      "[done] N=2: mean=0.0592s, 95% CI=(0.0528,0.0657), used=10/10\n",
      "[done] N=3: mean=0.0662s, 95% CI=(0.0636,0.0689), used=10/10\n",
      "[done] N=4: mean=0.0846s, 95% CI=(0.0767,0.0925), used=10/10\n",
      "[done] N=5: mean=0.0991s, 95% CI=(0.0964,0.1018), used=10/10\n",
      "[done] N=6: mean=0.1164s, 95% CI=(0.1078,0.1251), used=10/10\n",
      "[done] N=7: mean=0.1251s, 95% CI=(0.1220,0.1281), used=10/10\n",
      "[done] N=8: mean=0.1476s, 95% CI=(0.1438,0.1514), used=10/10\n",
      "[done] N=9: mean=0.1823s, 95% CI=(0.1729,0.1917), used=10/10\n",
      "[done] N=10: mean=0.2243s, 95% CI=(0.2064,0.2421), used=10/10\n",
      "[done] N=11: mean=0.2340s, 95% CI=(0.2242,0.2438), used=10/10\n",
      "[done] N=12: mean=0.2837s, 95% CI=(0.2688,0.2987), used=10/10\n",
      "[done] N=13: mean=0.2943s, 95% CI=(0.2832,0.3054), used=10/10\n",
      "[done] N=14: mean=0.3575s, 95% CI=(0.3465,0.3685), used=10/10\n",
      "[done] N=15: mean=0.3844s, 95% CI=(0.3752,0.3935), used=10/10\n",
      "[done] N=16: mean=0.4164s, 95% CI=(0.4060,0.4268), used=10/10\n",
      "[done] N=17: mean=0.4659s, 95% CI=(0.4458,0.4860), used=10/10\n",
      "[done] N=18: mean=0.5006s, 95% CI=(0.4941,0.5072), used=10/10\n",
      "[done] N=19: mean=0.5725s, 95% CI=(0.5510,0.5939), used=10/10\n",
      "[done] N=20: mean=0.6092s, 95% CI=(0.5938,0.6246), used=10/10\n",
      "[done] N=21: mean=0.6810s, 95% CI=(0.6294,0.7327), used=10/10\n",
      "[done] N=22: mean=0.8134s, 95% CI=(0.7972,0.8297), used=10/10\n",
      "[done] N=23: mean=0.9340s, 95% CI=(0.8574,1.0106), used=10/10\n",
      "[done] N=24: mean=0.9404s, 95% CI=(0.9224,0.9584), used=10/10\n",
      "[done] N=25: mean=1.0377s, 95% CI=(0.9964,1.0789), used=10/10\n",
      "[done] N=26: mean=1.1053s, 95% CI=(1.0889,1.1217), used=10/10\n",
      "[done] N=27: mean=1.2018s, 95% CI=(1.1650,1.2386), used=10/10\n",
      "[done] N=28: mean=1.3047s, 95% CI=(1.2695,1.3399), used=10/10\n",
      "[done] N=29: mean=1.3807s, 95% CI=(1.3611,1.4003), used=10/10\n",
      "[done] N=30: mean=1.4902s, 95% CI=(1.4643,1.5162), used=10/10\n",
      "[done] N=31: mean=1.5722s, 95% CI=(1.5573,1.5872), used=10/10\n",
      "[done] N=32: mean=1.7071s, 95% CI=(1.6533,1.7608), used=10/10\n",
      "[done] N=33: mean=1.8398s, 95% CI=(1.7840,1.8955), used=10/10\n",
      "[done] N=34: mean=1.9356s, 95% CI=(1.8850,1.9861), used=10/10\n",
      "[done] N=35: mean=2.0338s, 95% CI=(1.9803,2.0874), used=10/10\n",
      "[done] N=36: mean=2.1088s, 95% CI=(2.0825,2.1351), used=10/10\n",
      "[done] N=37: mean=2.2442s, 95% CI=(2.2271,2.2613), used=10/10\n",
      "[done] N=38: mean=2.7439s, 95% CI=(2.3990,3.0888), used=10/10\n",
      "[done] N=39: mean=2.4900s, 95% CI=(2.4624,2.5176), used=10/10\n",
      "[done] N=40: mean=2.7065s, 95% CI=(2.5815,2.8316), used=10/10\n",
      "[done] N=41: mean=2.7411s, 95% CI=(2.7082,2.7739), used=10/10\n",
      "[done] N=42: mean=2.9399s, 95% CI=(2.8832,2.9967), used=10/10\n",
      "[done] N=43: mean=3.0991s, 95% CI=(3.0697,3.1284), used=10/10\n",
      "[done] N=44: mean=3.2857s, 95% CI=(3.2367,3.3347), used=10/10\n",
      "[done] N=45: mean=3.4681s, 95% CI=(3.3494,3.5868), used=10/10\n",
      "[done] N=46: mean=3.5824s, 95% CI=(3.5220,3.6428), used=10/10\n",
      "[done] N=47: mean=3.7649s, 95% CI=(3.6990,3.8308), used=10/10\n",
      "[done] N=48: mean=3.9458s, 95% CI=(3.8805,4.0110), used=10/10\n",
      "[done] N=49: mean=4.2987s, 95% CI=(4.1160,4.4813), used=10/10\n",
      "[done] N=50: mean=4.8238s, 95% CI=(4.6551,4.9926), used=10/10\n",
      "[info] Saved: scalability_test_2_to_50\\runtime_repeats_long.csv\n",
      "[info] Saved: scalability_test_2_to_50\\runtime_summary.csv\n",
      "[info] Plot saved: scalability_test_2_to_50\\runtime_vs_accounts_CI.png\n",
      "[info] Plot saved: scalability_test_2_to_50\\loglog_fit.png\n",
      "[info] Regression summary saved.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Scalability test — single-period InfoPool (fixed τ, 20 assets) with uncertainty bands\n",
    "Scale N from 2 to 50 accounts; replicate accounts 11..50 from 1..10 (pattern n % 10).\n",
    "Add K repeated runs per N via small Dirichlet noise on initial holdings to quantify uncertainty.\n",
    "\n",
    "Outputs (in scalability_test_2_to_50/):\n",
    "  - runtime_repeats_long.csv   (N, rep, build_time, solve_time, total_time, status)\n",
    "  - runtime_summary.csv        (N, mean, std, ci_low, ci_high, slope_info)\n",
    "  - runtime_vs_accounts_CI.png (mean curve with 95% CI ribbon)\n",
    "  - loglog_fit.png             (log-log fit of time vs N with slope & 95% CI)\n",
    "  - _cache/  (pickle/npz for market data & Ω matrices; no pyarrow needed)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "import xpress as xp\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Paths & switches\n",
    "# -----------------------------\n",
    "OUTDIR = \"scalability_test_2_to_50\"\n",
    "CACHE_DIR = os.path.join(OUTDIR, \"_cache\")\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "ANNUAL_TRADING_DAYS = 252\n",
    "REPEATS = 10                \n",
    "DIRICHLET_ALPHA = 200.0      \n",
    "RANDOM_SEED = 20250101      \n",
    "\n",
    "# -----------------------------\n",
    "# Xpress init (guarded)\n",
    "# -----------------------------\n",
    "XPRESS_LICENSE_PATH = r\"C:/Users/s2667242/AppData/Local/anaconda3/Lib/site-packages/xpress/license/xpauth.xpr\"\n",
    "\n",
    "def ensure_xpress_inited():\n",
    "    if getattr(xp, \"_JYH_ALREADY_INITED\", False):\n",
    "        return\n",
    "    try:\n",
    "        xp.init(XPRESS_LICENSE_PATH)\n",
    "    except Exception as e:\n",
    "        print(\"[warn] xp.init() failed or already initialized, proceeding if environment is live:\", e)\n",
    "    try:\n",
    "        with xp.problem() as _p:\n",
    "            _p.setObjective(1.0, sense=xp.maximize)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\"Xpress global environment not available; restart kernel or fix license path.\") from e\n",
    "    xp._JYH_ALREADY_INITED = True\n",
    "\n",
    "ensure_xpress_inited()\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Market data & Ω-construction  (with pickle cache)\n",
    "# -----------------------------\n",
    "tickers = [\n",
    "    \"AAPL\",\"MSFT\",\"GOOGL\",\"AMZN\",\"META\",\"TSLA\",\"JNJ\",\"JPM\",\"V\",\"PG\",\n",
    "    \"NVDA\",\"DIS\",\"UNH\",\"HD\",\"MA\",\"PFE\",\"BAC\",\"KO\",\"XOM\",\"CVX\"\n",
    "]\n",
    "d = len(tickers)\n",
    "\n",
    "start_date = \"2020-01-01\"\n",
    "end_date   = \"2025-07-24\"\n",
    "\n",
    "prices_adj_pkl = os.path.join(CACHE_DIR, \"prices_adj.pkl\")\n",
    "prices_raw_pkl = os.path.join(CACHE_DIR, \"prices_raw.pkl\")\n",
    "vols_pkl       = os.path.join(CACHE_DIR, \"volumes.pkl\")\n",
    "omega_npz      = os.path.join(CACHE_DIR, \"omegas_corr.npz\")\n",
    "\n",
    "def winsorize_ser(s: pd.Series, q=0.995):\n",
    "    lo, hi = s.quantile(1-q), s.quantile(q)\n",
    "    return s.clip(lower=lo, upper=hi)\n",
    "\n",
    "def estimate_kappa_from_illiq(prices_raw, volumes, returns, sigma_vec, side=\"both\"):\n",
    "    px_raw = prices_raw.loc[returns.index]\n",
    "    vol    = volumes.loc[returns.index]\n",
    "    dollar_vol = (px_raw * vol).replace(0, np.nan)\n",
    "    illiq = (returns.abs() / dollar_vol).replace([np.inf, -np.inf], np.nan)\n",
    "    if side == \"plus\":\n",
    "        mask = (returns > 0)\n",
    "    elif side == \"minus\":\n",
    "        mask = (returns < 0)\n",
    "    else:\n",
    "        mask = pd.DataFrame(True, index=returns.index, columns=returns.columns)\n",
    "    illiq_med = winsorize_ser(illiq.where(mask).median(axis=0, skipna=True), q=0.995)\n",
    "    Vbar_local = (px_raw * vol).mean(axis=0)\n",
    "    sigma = pd.Series(sigma_vec, index=returns.columns)\n",
    "    kappa = (illiq_med * Vbar_local / sigma).replace([np.inf, -np.inf], np.nan)\n",
    "    return kappa.clip(lower=0.01, upper=100.0).values\n",
    "\n",
    "def gershgorin_delta_max(gamma: np.ndarray, corr: np.ndarray):\n",
    "    G = np.sqrt(np.outer(gamma, gamma))\n",
    "    A = np.abs(corr) * G\n",
    "    row_sum_off = A.sum(axis=1) - np.diag(A)\n",
    "    row_sum_off = np.where(row_sum_off <= 1e-12, np.inf, row_sum_off)\n",
    "    return float(np.min(gamma / row_sum_off))\n",
    "\n",
    "def delta_from_ratio(gamma: np.ndarray, corr: np.ndarray, rho=0.3, cap_ratio=0.9):\n",
    "    G = np.sqrt(np.outer(gamma, gamma))\n",
    "    A = np.abs(corr) * G\n",
    "    row_sum_off = A.sum(axis=1) - np.diag(A)\n",
    "    factor = row_sum_off / gamma\n",
    "    mf = np.mean(factor[np.isfinite(factor) & (factor > 0)])\n",
    "    delta_star = rho / mf if (mf is not None and mf > 0) else 0.0\n",
    "    delta_max  = gershgorin_delta_max(gamma, corr)\n",
    "    return float(min(delta_star, cap_ratio * delta_max)), float(delta_max)\n",
    "\n",
    "print(\"[info] Preparing market data and impact matrices (pickle cache) ...\")\n",
    "need_download = not (os.path.exists(prices_adj_pkl) and os.path.exists(prices_raw_pkl) and os.path.exists(vols_pkl))\n",
    "if need_download:\n",
    "    data = yf.download(tickers, start=start_date, end=end_date, auto_adjust=False, progress=False)\n",
    "    prices_adj = data[\"Adj Close\"]\n",
    "    prices_raw = data[\"Close\"]\n",
    "    volumes    = data[\"Volume\"]\n",
    "    prices_adj.to_pickle(prices_adj_pkl)\n",
    "    prices_raw.to_pickle(prices_raw_pkl)\n",
    "    volumes.to_pickle(vols_pkl)\n",
    "else:\n",
    "    prices_adj = pd.read_pickle(prices_adj_pkl)\n",
    "    prices_raw = pd.read_pickle(prices_raw_pkl)\n",
    "    volumes    = pd.read_pickle(vols_pkl)\n",
    "\n",
    "returns = prices_adj.pct_change().dropna()\n",
    "mean_daily_returns = returns.mean()\n",
    "cov_matrix         = returns.cov()\n",
    "\n",
    "sigma_all   = returns.std(axis=0).values\n",
    "up_returns  = returns.clip(lower=0)\n",
    "down_returns= (-returns).clip(lower=0)\n",
    "sigma_plus  = up_returns.std(axis=0).values\n",
    "sigma_minus = down_returns.std(axis=0).values\n",
    "\n",
    "dollar_vol = (prices_raw.reindex_like(volumes) * volumes).loc[returns.index]\n",
    "Vbar = dollar_vol.mean(axis=0).values\n",
    "\n",
    "if os.path.exists(omega_npz):\n",
    "    npz = np.load(omega_npz)\n",
    "    Omega_plus  = npz[\"Op\"]\n",
    "    Omega_minus = npz[\"Om\"]\n",
    "    corr        = npz[\"corr\"]\n",
    "else:\n",
    "    corr = cov_matrix.values / np.outer(sigma_all, sigma_all)\n",
    "    kappa_plus_hat  = estimate_kappa_from_illiq(prices_raw, volumes, returns, sigma_plus,  side=\"plus\")\n",
    "    kappa_minus_hat = estimate_kappa_from_illiq(prices_raw, volumes, returns, sigma_minus, side=\"minus\")\n",
    "    gamma_plus  = kappa_plus_hat  * sigma_plus  / Vbar\n",
    "    gamma_minus = kappa_minus_hat * sigma_minus / Vbar\n",
    "    delta_plus,  _ = delta_from_ratio(gamma_plus,  corr, rho=0.3, cap_ratio=0.9)\n",
    "    delta_minus, _ = delta_from_ratio(gamma_minus, corr, rho=0.3, cap_ratio=0.9)\n",
    "    Gp = np.sqrt(np.outer(gamma_plus,  gamma_plus))\n",
    "    Gm = np.sqrt(np.outer(gamma_minus, gamma_minus))\n",
    "    Omega_plus  = np.diag(gamma_plus)  + delta_plus  * (Gp * corr)\n",
    "    Omega_minus = np.diag(gamma_minus) + delta_minus * (Gm * corr)\n",
    "    # scale to target bps\n",
    "    xi = 0.10\n",
    "    target_bps = 100\n",
    "    c = target_bps / 1e4  # 100 bps = 0.01\n",
    "    s_cand_p = 2*c / np.maximum(gamma_plus  * xi * Vbar, 1e-30)\n",
    "    s_cand_m = 2*c / np.maximum(gamma_minus * xi * Vbar, 1e-30)\n",
    "    s_omega  = float(np.median(np.concatenate([s_cand_p, s_cand_m])))\n",
    "    Omega_plus  = s_omega * Omega_plus\n",
    "    Omega_minus = s_omega * Omega_minus\n",
    "    np.savez_compressed(omega_npz, Op=Omega_plus, Om=Omega_minus, corr=corr)\n",
    "\n",
    "for name, M in [(\"Omega_plus\", Omega_plus), (\"Omega_minus\", Omega_minus)]:\n",
    "    if not np.isfinite(M).all():\n",
    "        raise ValueError(f\"{name} contains non-finite values.\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Build up to 50 accounts by replicating the first 10\n",
    "# -----------------------------\n",
    "N_MAX = 50     # scale to 50\n",
    "BASE = 10      # replicate every 10 accounts (pattern by n % 10)\n",
    "\n",
    "gamma_core = [\n",
    "    0.49671415, 0.64768854, 0.76743473, 0.54256004, 0.24196227,\n",
    "    0.31424733, 0.06752820, 0.11092259, 0.37569802, 0.82254491\n",
    "]\n",
    "budgets_core = [1e6, 5e5, 8e5, 1.2e6, 6e5, 9e5, 7e5, 4e5, 1.5e6, 1e6]\n",
    "\n",
    "repeat_times = (N_MAX + BASE - 1) // BASE  # ceil(50/10)=5\n",
    "gamma_all   = (gamma_core   * repeat_times)[:N_MAX]\n",
    "budgets_all = (budgets_core * repeat_times)[:N_MAX]\n",
    "\n",
    "trading_days = ANNUAL_TRADING_DAYS\n",
    "mu_vec      = mean_daily_returns.values\n",
    "Sigma_mat   = cov_matrix.values\n",
    "mu_annual   = mu_vec * trading_days\n",
    "Sigma_annual= Sigma_mat * trading_days\n",
    "\n",
    "mu_list_ALL    = [mu_annual.copy()    for _ in range(N_MAX)]\n",
    "Sigma_list_ALL = [Sigma_annual.copy() for _ in range(N_MAX)]\n",
    "\n",
    "# risk scaling θ\n",
    "w_pct0 = np.full((N_MAX, d), 1.0/d)\n",
    "risk_proto = np.array([\n",
    "    0.5 * gamma_all[n] * (budgets_all[n]**2) * (w_pct0[n] @ Sigma_list_ALL[n] @ w_pct0[n])\n",
    "    for n in range(N_MAX)\n",
    "])\n",
    "rev_est = np.array([budgets_all[n] * (mu_list_ALL[n] @ w_pct0[n]) for n in range(N_MAX)])\n",
    "alpha_risk = 0.5\n",
    "s_theta = (alpha_risk * max(rev_est.mean(), 1e-12)) / max(risk_proto.mean(), 1e-12)\n",
    "theta_list_ALL = (s_theta * np.array(gamma_all)).tolist()\n",
    "\n",
    "# initial holdings: deterministic base-10 and replicate to 50\n",
    "INIT_HOLDINGS_PATH = os.path.join(CACHE_DIR, \"initial_holdings_50rep.csv\")\n",
    "if os.path.exists(INIT_HOLDINGS_PATH):\n",
    "    old_holdings_ALL = pd.read_csv(INIT_HOLDINGS_PATH, index_col=0).values\n",
    "    if old_holdings_ALL.shape != (N_MAX, d):\n",
    "        os.remove(INIT_HOLDINGS_PATH)\n",
    "        old_holdings_ALL = None\n",
    "else:\n",
    "    old_holdings_ALL = None\n",
    "\n",
    "def _dirichlet_like_from_hash(tickers, budgets, seed=12345):\n",
    "    Nn, dd = len(budgets), len(tickers)\n",
    "    W = np.zeros((Nn, dd), dtype=float)\n",
    "    for n in range(Nn):\n",
    "        e = np.empty(dd, dtype=float)\n",
    "        for j, t in enumerate(tickers):\n",
    "            key = f\"{seed}|acct={n}|ticker={t}\".encode()\n",
    "            h = hashlib.sha256(key).digest()\n",
    "            u64 = int.from_bytes(h[:8], \"big\")\n",
    "            u = (u64 + 0.5) / 2**64\n",
    "            u = min(max(u, 1e-12), 1-1e-12)\n",
    "            e[j] = -np.log(u)\n",
    "        w = e / e.sum()\n",
    "        W[n, :] = w\n",
    "    B = np.asarray(budgets, dtype=float)[:, None]\n",
    "    return W * B\n",
    "\n",
    "if old_holdings_ALL is None:\n",
    "    base10 = _dirichlet_like_from_hash(tickers, budgets_core, seed=12345)  # (10×20)\n",
    "    old_holdings_ALL = np.vstack([base10 for _ in range(repeat_times)])[:N_MAX, :]  # (50×20)\n",
    "    pd.DataFrame(old_holdings_ALL,\n",
    "                 index=[f\"Acct {i}\" for i in range(N_MAX)],\n",
    "                 columns=tickers).to_csv(INIT_HOLDINGS_PATH, float_format=\"%.6f\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Fixed τ (N×20): Acct 0 never shares; others share\n",
    "# -----------------------------\n",
    "TAU_BASE = np.ones((N_MAX, d), dtype=int)\n",
    "TAU_BASE[0, :] = 0\n",
    "pd.DataFrame(TAU_BASE, index=[f\"Acct {i}\" for i in range(N_MAX)], columns=tickers)\\\n",
    "  .to_csv(os.path.join(OUTDIR, \"tau_base_used.csv\"), float_format=\"%.0f\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Constraints replicator: account n uses pattern of (n % 10)\n",
    "# -----------------------------\n",
    "SECTOR_IDXS = {\n",
    "    \"T\":  [tickers.index(t) for t in [\"AAPL\",\"MSFT\",\"NVDA\"]],\n",
    "    \"C\":  [tickers.index(t) for t in [\"GOOGL\",\"META\",\"DIS\"]],\n",
    "    \"R\":  [tickers.index(t) for t in [\"AMZN\",\"HD\",\"TSLA\"]],\n",
    "    \"H\":  [tickers.index(t) for t in [\"JNJ\",\"UNH\",\"PFE\"]],\n",
    "    \"F\":  [tickers.index(t) for t in [\"JPM\",\"BAC\",\"V\",\"MA\"]],\n",
    "    \"S\":  [tickers.index(t) for t in [\"PG\",\"KO\"]],\n",
    "    \"E\":  [tickers.index(t) for t in [\"XOM\",\"CVX\"]],\n",
    "}\n",
    "GROWTH8 = [tickers.index(t) for t in [\"NVDA\",\"TSLA\",\"AAPL\",\"MSFT\"]]\n",
    "\n",
    "def add_constraints_for_account(model, n, w, budgets):\n",
    "    p = n % 10\n",
    "    if p == 0:\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in SECTOR_IDXS[\"S\"]+SECTOR_IDXS[\"H\"]) >=  budgets[n]*0.40)\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in SECTOR_IDXS[\"T\"]+SECTOR_IDXS[\"R\"]) <=  budgets[n]*0.25)\n",
    "    elif p == 1:\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in SECTOR_IDXS[\"T\"]) >=  budgets[n]*0.40)\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in SECTOR_IDXS[\"C\"]+[tickers.index(\"AMZN\")]) >= budgets[n]*0.30)\n",
    "    elif p == 2:\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in SECTOR_IDXS[\"T\"]) >=  budgets[n]*0.10)\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in SECTOR_IDXS[\"T\"]) <=  budgets[n]*0.25)\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in SECTOR_IDXS[\"C\"]) >=  budgets[n]*0.10)\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in SECTOR_IDXS[\"C\"]) <=  budgets[n]*0.25)\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in SECTOR_IDXS[\"H\"]) >=  budgets[n]*0.10)\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in SECTOR_IDXS[\"H\"]) <=  budgets[n]*0.25)\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in SECTOR_IDXS[\"F\"]) >=  budgets[n]*0.10)\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in SECTOR_IDXS[\"F\"]) <=  budgets[n]*0.25)\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in SECTOR_IDXS[\"S\"]+SECTOR_IDXS[\"E\"]) >= budgets[n]*0.05)\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in SECTOR_IDXS[\"S\"]+SECTOR_IDXS[\"E\"]) <= budgets[n]*0.15)\n",
    "    elif p == 3:\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in SECTOR_IDXS[\"S\"]+SECTOR_IDXS[\"F\"]+SECTOR_IDXS[\"E\"]) >= budgets[n]*0.50)\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in SECTOR_IDXS[\"F\"]) >=  budgets[n]*0.20)\n",
    "    elif p == 4:\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in SECTOR_IDXS[\"E\"]) >=  budgets[n]*0.15)\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in SECTOR_IDXS[\"F\"]) >=  budgets[n]*0.25)\n",
    "    elif p == 5:\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in SECTOR_IDXS[\"T\"]) >=  budgets[n]*0.30)\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in SECTOR_IDXS[\"H\"]) >=  budgets[n]*0.20)\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in SECTOR_IDXS[\"S\"]+SECTOR_IDXS[\"R\"]) <= budgets[n]*0.15)\n",
    "    elif p == 6:\n",
    "        growth6 = SECTOR_IDXS[\"T\"] + SECTOR_IDXS[\"C\"] + SECTOR_IDXS[\"R\"]\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in growth6) <= budgets[n]*0.35)\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in SECTOR_IDXS[\"E\"]+SECTOR_IDXS[\"F\"]+SECTOR_IDXS[\"S\"]) >= budgets[n]*0.40)\n",
    "    elif p == 7:\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in [tickers.index(\"AMZN\"), tickers.index(\"TSLA\")]) >= budgets[n]*0.30)\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in SECTOR_IDXS[\"T\"]+SECTOR_IDXS[\"F\"]+SECTOR_IDXS[\"H\"]) <= budgets[n]*0.50)\n",
    "    elif p == 8:\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in GROWTH8) >= budgets[n]*0.50)\n",
    "    elif p == 9:\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in SECTOR_IDXS[\"S\"]+SECTOR_IDXS[\"E\"]) >= budgets[n]*0.20)\n",
    "        model.addConstraint(xp.Sum(w[n][j] for j in SECTOR_IDXS[\"T\"]+SECTOR_IDXS[\"C\"]+SECTOR_IDXS[\"R\"]) >= budgets[n]*0.30)\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Helpers: randomize holdings & CI computation\n",
    "# -----------------------------\n",
    "rng_global = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "def randomize_holdings_dirichlet(base_holdings, budgets, alpha=DIRICHLET_ALPHA, rng=None):\n",
    "    \"\"\"\n",
    "    base_holdings: (N × d) base dollar holdings\n",
    "    budgets:       (N,)   dollar budgets\n",
    "    Returns: (N × d) new holdings with small Dirichlet noise preserving budgets.\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = rng_global\n",
    "    N, d_ = base_holdings.shape\n",
    "    assert d_ == d\n",
    "    W = np.zeros_like(base_holdings, dtype=float)\n",
    "    for n in range(N):\n",
    "        w_base = base_holdings[n] / max(budgets[n], 1e-30)\n",
    "        w_base = np.maximum(w_base, 1e-12)\n",
    "        w_base = w_base / w_base.sum()\n",
    "        # Dirichlet(alpha * w_base)\n",
    "        a = alpha * w_base\n",
    "        draw = rng.dirichlet(a)\n",
    "        W[n] = draw * budgets[n]\n",
    "    return W\n",
    "\n",
    "def mean_ci(x, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Return mean, std, and (1-alpha) CI using normal approx: mean ± z * std/sqrt(n)\n",
    "    \"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    x = x[np.isfinite(x)]\n",
    "    n = len(x)\n",
    "    if n == 0:\n",
    "        return np.nan, np.nan, np.nan, np.nan\n",
    "    m = float(np.mean(x))\n",
    "    s = float(np.std(x, ddof=1)) if n > 1 else 0.0\n",
    "    z = 1.96  # ~95%\n",
    "    half = z * s / np.sqrt(max(n, 1))\n",
    "    return m, s, m - half, m + half\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Core solver (build-time & solve-time separated)\n",
    "# -----------------------------\n",
    "def solve_infopool_once(N: int, tau_mat: np.ndarray,\n",
    "                        budgets, mu_list, Sigma_list,\n",
    "                        Omega_plus, Omega_minus,\n",
    "                        old_holdings, theta_list):\n",
    "    assert tau_mat.shape == (N, d)\n",
    "    assert old_holdings.shape == (N, d)\n",
    "\n",
    "    with xp.problem() as model:\n",
    "        model.controls.outputlog = 0  # faster\n",
    "        # model.controls.threads = -1  # optional\n",
    "\n",
    "        tb0 = time.perf_counter()\n",
    "\n",
    "        # variables\n",
    "        w   = [[model.addVariable(lb=0.0, ub=budgets[n]) for j in range(d)] for n in range(N)]\n",
    "        wp  = [[model.addVariable(lb=0.0, ub=budgets[n]) for j in range(d)] for n in range(N)]\n",
    "        wm  = [[model.addVariable(lb=0.0, ub=budgets[n]) for j in range(d)] for n in range(N)]\n",
    "        w_pct = [[model.addVariable(lb=0.0, ub=1.0)     for j in range(d)] for n in range(N)]\n",
    "\n",
    "        # buy/sell split\n",
    "        for n in range(N):\n",
    "            for j in range(d):\n",
    "                model.addConstraint(wp[n][j] - wm[n][j] == w[n][j] - old_holdings[n][j])\n",
    "\n",
    "        # budgets + simplex\n",
    "        for n in range(N):\n",
    "            model.addConstraint(xp.Sum(w[n][j] for j in range(d)) == budgets[n])\n",
    "            for j in range(d):\n",
    "                model.addConstraint(w[n][j] == budgets[n] * w_pct[n][j])\n",
    "            model.addConstraint(xp.Sum(w_pct[n][j] for j in range(d)) == 1)\n",
    "\n",
    "        # constraints pattern\n",
    "        for n in range(N):\n",
    "            add_constraints_for_account(model, n, w, budgets)\n",
    "\n",
    "        # objective\n",
    "        lin = xp.Sum(mu_list[n][j] * w[n][j] for n in range(N) for j in range(d))\n",
    "        risk = xp.Sum(\n",
    "            -0.5 * theta_list[n] * (budgets[n]**2) *\n",
    "            xp.Sum(Sigma_list[n][i][j] * w_pct[n][i] * w_pct[n][j] for i in range(d) for j in range(d))\n",
    "            for n in range(N)\n",
    "        )\n",
    "\n",
    "        tau_arr = np.asarray(tau_mat, dtype=int)\n",
    "        T_plus  = [xp.Sum(tau_arr[k, j] * wp[k][j]  for k in range(N)) for j in range(d)]\n",
    "        T_minus = [xp.Sum(tau_arr[k, j] * wm[k][j] for k in range(N)) for j in range(d)]\n",
    "\n",
    "        quad_pp = xp.Sum(T_plus[i]  * xp.Sum(Omega_plus[i,  j] * T_plus[j]  for j in range(d)) for i in range(d))\n",
    "        quad_mm = xp.Sum(T_minus[i] * xp.Sum(Omega_plus[i,  j] * T_minus[j] for j in range(d)) for i in range(d))\n",
    "        cross_pm= xp.Sum(T_plus[i]  * xp.Sum(Omega_minus[i, j] * T_minus[j] for j in range(d)) for i in range(d))\n",
    "        impact_pool = 0.5 * (quad_pp + quad_mm + 2 * cross_pm)\n",
    "\n",
    "        impact_ind = 0\n",
    "        for m in range(N):\n",
    "            r_p = [ (1 - tau_arr[m, j]) * wp[m][j]  for j in range(d) ]\n",
    "            r_m = [ (1 - tau_arr[m, j]) * wm[m][j] for j in range(d) ]\n",
    "            quad_pp_m = xp.Sum(r_p[i] * xp.Sum(Omega_plus[i,  j] * r_p[j] for j in range(d)) for i in range(d))\n",
    "            quad_mm_m = xp.Sum(r_m[i] * xp.Sum(Omega_plus[i,  j] * r_m[j] for j in range(d)) for i in range(d))\n",
    "            cross_pm_m= xp.Sum(r_p[i] * xp.Sum(Omega_minus[i, j] * r_m[j] for j in range(d)) for i in range(d))\n",
    "            impact_ind += 0.5 * (quad_pp_m + quad_mm_m + 2.0 * cross_pm_m)\n",
    "\n",
    "        impact = - ANNUAL_TRADING_DAYS * (impact_pool + impact_ind)\n",
    "        model.setObjective(lin + risk + impact, sense=xp.maximize)\n",
    "\n",
    "        tb1 = time.perf_counter()\n",
    "        model.solve()\n",
    "        ts1 = time.perf_counter()\n",
    "\n",
    "        build_time = tb1 - tb0\n",
    "        solve_time = ts1 - tb1\n",
    "\n",
    "    return build_time, solve_time\n",
    "\n",
    "# -----------------------------\n",
    "# 7) Run scalability sweep with repeats: N = 2..50\n",
    "# -----------------------------\n",
    "runtime_long = []  # per-repeat records\n",
    "summary_rows = []\n",
    "\n",
    "TAU_FULL = TAU_BASE.copy()\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "print(\"\\n[info] Starting scalability sweep with repeats (N = 2..50, K = %d) ...\" % REPEATS)\n",
    "for N in range(2, N_MAX+1):\n",
    "    budgets     = budgets_all[:N]\n",
    "    mu_list     = mu_list_ALL[:N]\n",
    "    Sigma_list  = Sigma_list_ALL[:N]\n",
    "    theta_list  = theta_list_ALL[:N]\n",
    "    tau_N       = TAU_FULL[:N, :].copy()\n",
    "\n",
    "    \n",
    "    base_holdings_N = old_holdings_ALL[:N, :]\n",
    "\n",
    "    times_total = []\n",
    "    for rep in range(REPEATS):\n",
    "       \n",
    "        seed_rep = rng.integers(0, 2**32-1, dtype=np.uint64)\n",
    "        rng_rep = np.random.default_rng(int(seed_rep))\n",
    "        noisy_holdings = randomize_holdings_dirichlet(\n",
    "            base_holdings_N, budgets,\n",
    "            alpha=DIRICHLET_ALPHA, rng=rng_rep\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            build_t, solve_t = solve_infopool_once(\n",
    "                N, tau_N, budgets, mu_list, Sigma_list,\n",
    "                Omega_plus, Omega_minus, noisy_holdings, theta_list\n",
    "            )\n",
    "            total_t = build_t + solve_t\n",
    "            status = \"ok\"\n",
    "        except Exception as e:\n",
    "            build_t = np.nan; solve_t = np.nan; total_t = np.nan\n",
    "            status = f\"error: {e}\"\n",
    "            print(f\"[ERR ] N={N}, rep={rep}: {e}\")\n",
    "\n",
    "        runtime_long.append({\n",
    "            \"N\": N, \"rep\": rep,\n",
    "            \"build_time_sec\": build_t,\n",
    "            \"solve_time_sec\": solve_t,\n",
    "            \"total_time_sec\": total_t,\n",
    "            \"status\": status\n",
    "        })\n",
    "        if np.isfinite(total_t):\n",
    "            times_total.append(total_t)\n",
    "\n",
    "    m, s, lo, hi = mean_ci(times_total, alpha=0.05)\n",
    "    summary_rows.append({\n",
    "        \"N\": N,\n",
    "        \"mean_total_sec\": m,\n",
    "        \"std_total_sec\": s,\n",
    "        \"ci95_low\": lo,\n",
    "        \"ci95_high\": hi,\n",
    "        \"repeats_used\": len(times_total)\n",
    "    })\n",
    "    print(f\"[done] N={N}: mean={m:.4f}s, 95% CI=({lo:.4f},{hi:.4f}), used={len(times_total)}/{REPEATS}\")\n",
    "\n",
    "# Save long-form & summary\n",
    "long_df = pd.DataFrame(runtime_long)\n",
    "long_path = os.path.join(OUTDIR, \"runtime_repeats_long.csv\")\n",
    "long_df.to_csv(long_path, index=False)\n",
    "print(\"[info] Saved:\", long_path)\n",
    "\n",
    "summ_df = pd.DataFrame(summary_rows)\n",
    "summ_path = os.path.join(OUTDIR, \"runtime_summary.csv\")\n",
    "summ_df.to_csv(summ_path, index=False)\n",
    "print(\"[info] Saved:\", summ_path)\n",
    "\n",
    "# -----------------------------\n",
    "# 8) Visualization: mean curve + 95% CI ribbon\n",
    "# -----------------------------\n",
    "ok_df = summ_df[np.isfinite(summ_df[\"mean_total_sec\"])].copy()\n",
    "xN = ok_df[\"N\"].values\n",
    "y  = ok_df[\"mean_total_sec\"].values\n",
    "lo = ok_df[\"ci95_low\"].values\n",
    "hi = ok_df[\"ci95_high\"].values\n",
    "\n",
    "plt.figure(figsize=(7.6, 4.6))\n",
    "plt.plot(xN, y, marker=\"o\", label=\"Mean total time\")\n",
    "plt.fill_between(xN, lo, hi, alpha=0.25, label=\"95% CI\")\n",
    "plt.xlabel(\"Number of Accounts (N)\")\n",
    "plt.ylabel(\"Total Time (seconds)\")\n",
    "plt.title(\"InfoPool Single-Period Scalability with Uncertainty (Assets = 20): N=2..50\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.legend()\n",
    "png_path = os.path.join(OUTDIR, \"runtime_vs_accounts_CI.png\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(png_path, dpi=180)\n",
    "plt.close()\n",
    "print(\"[info] Plot saved:\", png_path)\n",
    "\n",
    "# -----------------------------\n",
    "# 9) log-log regression: slope & 95% CI (analytical)\n",
    "# -----------------------------\n",
    "# 只对正值做回归\n",
    "mask = (y > 0) & (xN > 0)\n",
    "x = np.log(xN[mask])\n",
    "t = np.log(y[mask])\n",
    "\n",
    "# OLS: t = a + b * x + eps\n",
    "X = np.vstack([np.ones_like(x), x]).T\n",
    "beta_hat = np.linalg.lstsq(X, t, rcond=None)[0]  # [a, b]\n",
    "resid = t - X @ beta_hat\n",
    "n, k = X.shape\n",
    "sigma2 = (resid @ resid) / (n - k)\n",
    "cov_beta = sigma2 * np.linalg.inv(X.T @ X)\n",
    "se_b = np.sqrt(cov_beta[1, 1])\n",
    "b = float(beta_hat[1])\n",
    "# 95% CI for slope using normal approx\n",
    "b_lo = b - 1.96 * se_b\n",
    "b_hi = b + 1.96 * se_b\n",
    "\n",
    "# 画图\n",
    "xx = np.linspace(x.min(), x.max(), 200)\n",
    "tt = beta_hat[0] + beta_hat[1] * xx\n",
    "plt.figure(figsize=(7.0, 4.4))\n",
    "plt.scatter(x, t, s=20, alpha=0.7, label=\"Data (log-log)\")\n",
    "plt.plot(xx, tt, lw=2, label=f\"OLS fit: slope={b:.3f} (95% CI [{b_lo:.3f},{b_hi:.3f}])\")\n",
    "plt.xlabel(\"log N\")\n",
    "plt.ylabel(\"log Time\")\n",
    "plt.title(\"Log–Log Fit of Runtime vs. N\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.legend()\n",
    "logfit_path = os.path.join(OUTDIR, \"loglog_fit.png\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(logfit_path, dpi=180)\n",
    "plt.close()\n",
    "print(\"[info] Plot saved:\", logfit_path)\n",
    "\n",
    "# 附加写入回归结果\n",
    "with open(os.path.join(OUTDIR, \"scalability_regression.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"log-log OLS slope (time vs N): {b:.6f}\\n\")\n",
    "    f.write(f\"95% CI: [{b_lo:.6f}, {b_hi:.6f}]\\n\")\n",
    "    f.write(f\"n={n}, residual variance={sigma2:.6e}\\n\")\n",
    "print(\"[info] Regression summary saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09542653-8911-4c39-8de6-fc52d2993729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominance report (single-asset points):\n",
      "- AAPL: DOMINATED by MSFT\n",
      "- AMZN: DOMINATED by AAPL, GOOGL, MSFT\n",
      "- BAC: DOMINATED by AAPL, CVX, GOOGL, HD, JPM, MA, MSFT, V, XOM\n",
      "- CVX: DOMINATED by AAPL, GOOGL, HD, JPM, MA, MSFT, V, XOM\n",
      "- DIS: DOMINATED by AAPL, GOOGL, HD, JNJ, JPM, KO, MA, MSFT, PG, UNH, V, XOM\n",
      "- GOOGL: DOMINATED by AAPL, MSFT\n",
      "- HD: NOT dominated\n",
      "- JNJ: NOT dominated\n",
      "- JPM: DOMINATED by MSFT\n",
      "- KO: NOT dominated\n",
      "- MA: DOMINATED by MSFT\n",
      "- META: NOT dominated\n",
      "- MSFT: NOT dominated\n",
      "- NVDA: NOT dominated\n",
      "- PFE: DOMINATED by JNJ, KO, PG\n",
      "- PG: DOMINATED by KO\n",
      "- TSLA: DOMINATED by NVDA\n",
      "- UNH: DOMINATED by AAPL, GOOGL, HD, JNJ, JPM, KO, MA, MSFT, PG, V\n",
      "- V: NOT dominated\n",
      "- XOM: DOMINATED by AAPL, GOOGL, JPM, MSFT\n",
      "\n",
      "Saved plot: infopool_batch_outputs\\assets_efficient_frontier_20.png\n",
      "Saved table: infopool_batch_outputs\\assets_eff_summary_20.csv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Efficient frontier (single-asset points, 20 names)\n",
    "- Dominated assets: hollow markers\n",
    "- Non-dominated assets: solid markers\n",
    "- Prints, for each asset, whether it is dominated and by which assets\n",
    "- Saves PNG + CSV to infopool_batch_outputs/\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Paths & constants\n",
    "# -----------------------------\n",
    "OUTDIR = \"infopool_batch_outputs\"\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "ANNUAL_TRADING_DAYS = 252\n",
    "\n",
    "TICKERS = [\n",
    "    \"AAPL\",\"MSFT\",\"GOOGL\",\"AMZN\",\"META\",\"TSLA\",\"JNJ\",\"JPM\",\"V\",\"PG\",\n",
    "    \"NVDA\",\"DIS\",\"UNH\",\"HD\",\"MA\",\"PFE\",\"BAC\",\"KO\",\"XOM\",\"CVX\"\n",
    "]\n",
    "START_DATE = \"2020-01-01\"\n",
    "END_DATE   = \"2025-07-24\"\n",
    "\n",
    "PNG_PATH = os.path.join(OUTDIR, \"assets_efficient_frontier_20.png\")\n",
    "CSV_PATH = os.path.join(OUTDIR, \"assets_eff_summary_20.csv\")\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Download prices and build daily returns\n",
    "#    NOTE: auto_adjust=False so 'Adj Close' is available.\n",
    "#          Fallback to 'Close' if 'Adj Close' is missing.\n",
    "# -----------------------------\n",
    "data = yf.download(\n",
    "    TICKERS, start=START_DATE, end=END_DATE,\n",
    "    auto_adjust=False, progress=False\n",
    ")\n",
    "\n",
    "if isinstance(data.columns, pd.MultiIndex):\n",
    "    lvl0 = data.columns.get_level_values(0)\n",
    "    if \"Adj Close\" in set(lvl0):\n",
    "        prices = data[\"Adj Close\"]\n",
    "    elif \"Close\" in set(lvl0):\n",
    "        prices = data[\"Close\"]\n",
    "    else:\n",
    "        raise KeyError(f\"No 'Adj Close' or 'Close' in columns. Top-level fields found: {sorted(set(lvl0))}\")\n",
    "else:\n",
    "    # Rare case: single-level columns; try to pick a sensible field\n",
    "    if \"Adj Close\" in data.columns:\n",
    "        prices = data[[\"Adj Close\"]]\n",
    "    elif \"Close\" in data.columns:\n",
    "        prices = data[[\"Close\"]]\n",
    "    else:\n",
    "        raise KeyError(f\"No 'Adj Close' or 'Close' in columns: {list(data.columns)}\")\n",
    "\n",
    "prices = prices.dropna(how=\"all\")\n",
    "returns = prices.pct_change().dropna()\n",
    "\n",
    "# If single-level (one column) because of unexpected format, reshape to wide with tickers.\n",
    "if returns.shape[1] == 1 and len(TICKERS) > 1:\n",
    "    # Last-resort: refetch with auto_adjust=True and columns=tickers (yfinance sometimes flattens)\n",
    "    data2 = yf.download(TICKERS, start=START_DATE, end=END_DATE, auto_adjust=True, progress=False)\n",
    "    if isinstance(data2.columns, pd.MultiIndex) and \"Close\" in set(data2.columns.get_level_values(0)):\n",
    "        prices2 = data2[\"Close\"]\n",
    "    else:\n",
    "        # With auto_adjust=True, 'Close' is adjusted; for multi-index it’s usually 'Close'\n",
    "        # For single-level, columns should be tickers already\n",
    "        prices2 = data2.copy()\n",
    "    prices2 = prices2.dropna(how=\"all\")\n",
    "    returns = prices2.pct_change().dropna()\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Annualized mean (μ) and volatility (σ) per asset\n",
    "# -----------------------------\n",
    "mu_daily = returns.mean()\n",
    "sig_daily = returns.std()\n",
    "\n",
    "mu_annual = mu_daily * ANNUAL_TRADING_DAYS\n",
    "sig_annual = sig_daily * np.sqrt(ANNUAL_TRADING_DAYS)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"ticker\": mu_annual.index,\n",
    "    \"mu_annual\": mu_annual.values,\n",
    "    \"sigma_annual\": sig_annual.values\n",
    "}).set_index(\"ticker\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Dominance test among single-asset points\n",
    "#    i is dominated if ∃ j: sigma_j <= sigma_i and mu_j >= mu_i (with ≥ in both and at least one strict)\n",
    "# -----------------------------\n",
    "dominators = {t: [] for t in df.index}\n",
    "for i in df.index:\n",
    "    mu_i, s_i = df.loc[i, \"mu_annual\"], df.loc[i, \"sigma_annual\"]\n",
    "    for j in df.index:\n",
    "        if j == i:\n",
    "            continue\n",
    "        mu_j, s_j = df.loc[j, \"mu_annual\"], df.loc[j, \"sigma_annual\"]\n",
    "        weak = (s_j <= s_i) and (mu_j >= mu_i)\n",
    "        strict = (s_j < s_i) or (mu_j > mu_i)\n",
    "        if weak and strict:\n",
    "            dominators[i].append(j)\n",
    "\n",
    "df[\"dominated\"]  = [len(dominators[t]) > 0 for t in df.index]\n",
    "df[\"dominators\"] = [dominators[t] for t in df.index]\n",
    "\n",
    "# Upper envelope (optional)\n",
    "nd_df = df[~df[\"dominated\"]].copy().sort_values(\"sigma_annual\")\n",
    "frontier = []\n",
    "last_mu = -np.inf\n",
    "for t, row in nd_df.iterrows():\n",
    "    if row[\"mu_annual\"] > last_mu + 1e-15:\n",
    "        frontier.append((row[\"sigma_annual\"], row[\"mu_annual\"]))\n",
    "        last_mu = row[\"mu_annual\"]\n",
    "frontier = np.array(frontier)\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Plot\n",
    "# -----------------------------\n",
    "plt.figure(figsize=(7.8, 5.0))\n",
    "\n",
    "# Dominated: hollow markers\n",
    "dom = df[df[\"dominated\"]]\n",
    "plt.scatter(\n",
    "    dom[\"sigma_annual\"], dom[\"mu_annual\"],\n",
    "    s=64, facecolors=\"none\", edgecolors=\"black\", linewidths=1.5, label=\"Dominated\"\n",
    ")\n",
    "\n",
    "# Non-dominated: solid markers\n",
    "nd = df[~df[\"dominated\"]]\n",
    "plt.scatter(\n",
    "    nd[\"sigma_annual\"], nd[\"mu_annual\"],\n",
    "    s=64, color=\"black\", label=\"Non-dominated\"\n",
    ")\n",
    "\n",
    "# Frontier line\n",
    "if frontier.size >= 2:\n",
    "    plt.plot(frontier[:, 0], frontier[:, 1], \"-\", linewidth=1.6, color=\"black\", alpha=0.85)\n",
    "\n",
    "# Light labels for non-dominated only\n",
    "for t, row in nd.iterrows():\n",
    "    plt.annotate(t, (row[\"sigma_annual\"], row[\"mu_annual\"]),\n",
    "                 xytext=(4, 4), textcoords=\"offset points\", fontsize=9)\n",
    "\n",
    "plt.title(\"Single-Asset Efficient Frontier (Annualized, 20 Assets)\")\n",
    "plt.xlabel(\"Volatility σ (annualized)\")\n",
    "plt.ylabel(\"Expected Return μ (annualized)\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(PNG_PATH, dpi=200)\n",
    "plt.close()\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Save CSV + print dominance report\n",
    "# -----------------------------\n",
    "df_out = df.copy()\n",
    "df_out[\"dominators\"] = df_out[\"dominators\"].apply(lambda L: \",\".join(L) if len(L) else \"\")\n",
    "df_out.to_csv(CSV_PATH, float_format=\"%.6f\")\n",
    "\n",
    "print(\"Dominance report (single-asset points):\")\n",
    "for t in df.index:\n",
    "    if df.loc[t, \"dominated\"]:\n",
    "        print(f\"- {t}: DOMINATED by {', '.join(dominators[t])}\")\n",
    "    else:\n",
    "        print(f\"- {t}: NOT dominated\")\n",
    "\n",
    "print(f\"\\nSaved plot: {PNG_PATH}\")\n",
    "print(f\"Saved table: {CSV_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc1f784-90b1-4d44-b82a-2932a83d62f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
